Dobrý den, ahoj. Vítám vás, přátelé, u dalšího modulu programu Future AI Leader, tentokrát na téma jak na chytrou práci s daty. A tady u toho modulu mám na začátek důležité upozornění. Já nejsem datový analitik, jsou mezi námi lidé, kteří jsou specializovaní v této profesi nebo kteří s tím mají daleko větší zkušenosti. Já však věřím, že práce s daty je na určité úrovni opravdu základním předpokledem toho, abychom jednak si uspořádali digitální život, abychom dokázali využít plný potenciál umělé inteligence. Práce s daty je velmi široký obor, ať už je to samotná datová analitika, business intelligence, různé etické právní aspekty a tak dál. A samozřejmě není v naší moci obsáhnout všechno. Já se proto budu soustředit na velmi praktické využití AI při práci s daty a to v práci znalostních kreativních pracovníků. To znamená to, co děláme při práci my všichni v rámci failu. Takže to je jenom upozornění na začátek. V dnešním přípravě se podíváme na data, pak na scénáře, jak se dají využít digitální nástroje a nakonec se budeme bavit o AI. Možnosti použití nástroje, ale taky limity. Protože kolem nás vidím hodně takových různých demo-ukázek, jak se dá možná s daty pracovat, ale některá demo jsou poměrně daleko od reality a musíme si na to dát pozor. Takže i tohoto tématu se dotkneme. Takže jdeme na to. Začnu takovými základními poznatky, které se týkají dnešních dat. Já jsem kdyžsi viděl startup, který měl na titulní stránce svého webových stránek, že data jsou aktivům. Znáte jejich hodnotu? Já jsem si uvědomil, že my o datech hodně mluvíme. Že se říká, že data jsou nová ropa, že právě v datech je ta největší hodnota, ale já jsem si uvědomil, že častokrát neznáme jejich hodnotu a že ta hodnota nespočívá častokrát v samotných datech, ale v tom, jestli je dokážeme využít. A tady si myslím, že přichází naprosto jedinečná příležitost, abychom všechna ta data, která ve firmě máme, dokázali využít daleko lépe než před příchodem AI. Každopádně ve firmách těch dat je opravdu hodně. Neustále se nějaká data sbírají, evidují od kontaktu na zákazníky, přes diskuze na videohovorech. Toto všechno generuje data a většina firm, aspoň z mé zkušenosti, ta data nevyužívá tak, jak by měla. Firmy se častokrát spoléhají na experty, takže vytváří specializované oddělení nebo týmy pro datovou analitiku. V řadě případů to dává velký smysl. Já osobně věřím tomu, že by se data měly přiblížit biznisu, že ta datová oddělení by měla spíše se starat o ty trubky, aby data byly v pořádku, byly k dispozici. Dali se lidem nástroje, zpřístupnili se data a aby samotný biznis se s nimi naučil pracovat. Kdo by měl udělat rychlou analýzu potenciálních klientů, kteří jsou už v databázi, ale dlouho nenakoupili? Je daleko lepší, když to udělá obchodník, než když bude muset posílat někam požadavek. Já osobně věřím v demokratizaci dat a v tom, že v té správné kombinaci data a nástroj to vytváří biznisovou hodnotu. Častokrát se ale setkávám s tím, že mi ve firmách lidé říkají, že máme v datech nepořádek. Přátelé to mají úplně všichni. Toto nesmí být překážka toho, abychom se daty zabývali. A taky můžeme pomoct tomu, abychom si data daly do pořádku, vyčistili je, připravili je na dobu AI agentů právě ve spolupráci s AI. Takže je to opravdu o hodnotě a o tom, jak z těch dat vytěžit něco víc, než to, co mám dneska. Těch dat existuje celé množství řad, kategorii a tak dále. Máme nestrukturovaná data, máme syntetická data, vygenerovaná AI, máme nejenom data v podobě různých textů a záznamů, ale dneska video, audio. Toto všechno jsou data, která můžeme za pomocí AI zpracovávat. Pro ty naše účely se budeme primárně zabývat dvěmi kategoriemi. Strukturovaná a nestrukturovaná data. Strukturovaná data jsou cokoliv, co máte uloženo v nějakých databázích, ať už v tabulkách, v informačních systémech. Jsou to data, která jsou popsaná, mají stejné kategorie, nevyžadují tolik paměti, dají se s nimi pracovat trošku jiným způsobem, než s těmi nestrukturovanými. Každopádně některé analýzy a studie říkají, že jedna z těch strukturovaných dát máme ve firmách pouze zhruba 20 %. Ve vašem případě to mohou být kontakty na zákazníky, může to být registrační formulář, přes který sbíráte, přihlášené účastníky vašich vzdělávacích programů. Zkrátka to, co máme v databázích, jsou strukturovaná data. Oproti tomu nestrukturovaná data mohou být texty, smlouvy, obrázky, prezentace. Zkrátka všechno, co máme mimo tady ty systémy, předpokládá se, že jich je logicky zhruba 80 %, a ještě donedávna bylo poměrně náročné s nimi pracovat. S příchodem AI se však toto změnilo. A já osobně věřím, že častokrát právě v těchto informacích, v těchto datech se sklývá ta největší hodnota. I proto někdy říkám, že je možné, že největší hodnota leží v našich konverzacích. My jsme to téma otevřeli už při diskuzi o druhém digitálním mozku. I proto se v pátek budeme bavit právě nad tím, jak ta data připravovat, chystat pro použití v druhém digitálním mozku. Nicméně, když se bavíme o konverzacích, tak tam z těch dat můžeme vytáhnout cenné poznatky, cenné informace a hlavně cenné znalosti. A ty znalosti pak můžeme použít ať už pro nějaký jednorázový report, nebo pro design a stavění AI assistentů a agentů. I proto věřím, že bychom měli ty konverzace nahrávat a naučit se analyzovat tak, abychom z nich ty znalosti dokázali vytáhnout. Častokrát se dnes používá i pojem context engineering. Je to takový doplňek relativně nová věc vedle prompt engineeringu, kdy prompt pro AI je instrukce, kdy zadám AI, co má udělat a AI to udělá. Jakmile se začaly rozvíjet komplexnější modely, my jsme začali chtít, aby dělali na komplexnějších věcech, tak se ukázalo, že nestačí jenom ten systémový, nebo ten náš uživatelský prompt. Ale chceme, potřebujeme, ale hlavně chceme, dát AI nějaký kontext. A ten kontext může být vaše interní znalost či informace, můžou to být nějaká data z internetu, může to být i nějaký nástroj, aby si třeba ta data stáhl, nebo aby něco naprogramoval. A tady ta kombinace prompt a kontext je podle mě klíčem k úspěšnému využití AI. Dá se říct, že i to, co děláme my lidé, je prompt, to je ta instrukce, ta akce, kterou děláme, a kontext. To jsou naše znalosti, schopnost nějaké analýzy, syntézy. Takže jakmile jste schopni designovat dobré prompty a připravit dobrý kontext, to znamená data, dá se říct, že jste ready a připraveni na to, abyste dokázali AI využít. Nicméně, co to znamená prakticky? Já jsem už asi 20x použil slovo data. Pro mě je to o scenářích. Scenářích toho, kdy čím dál víc potřebuju dávat AI nějaký kontext, analyzovat to, co děláme, jak to děláme, a na základě toho stavět další věci. To znamená, tady těch příkladů vidíte deset, mohl bych jich ukázat daleko víc, ale může to být analýza zpětné vazby o zákazníku. Ať už zpětné vazby, kterou dostáváme třeba v rámci nějakého dotazníku, ale třeba i zpětné vazby, kterou lidé dávají do hodnocení na Google Maps. Když si stáhnu tady to hodnocení, je jedno, jestli je jich sto nebo deset tisíc, za pomocí AI to dnes dokážu analyzovat. Může to být třeba čištění a organizace kontaktů. Já jsem vám tady ty use case čištění už ukazoval, ale podíváme se na ně v pátek. To znamená, máme data v CRM-ku, nebo chci si udělat personalizaci LinkedIn kontaktů, nebo jenom vyčistit telefonní čísla. Na to se opět dá krásně použít AI. No ale jsou to také všechny reportingy, které se opakují. To znamená report o marketingových aktivitách, o výkonu naší obchodní sítě. Toto všechno jsou aktivity, které jsou spojeny s datovou analýzou a je dobré poznat, jaké možnosti nám technologie nabízejí. Já osobně to ještě rozděluji na takové dvě oblasti. Ta první je jednorázová. To znamená, jsou to aktivity, které nedělám pravidelně, kdybych měl znát možnosti těch jednotlivých nástrojů a vědět, kdy jaký použít. Zrovna v tomto se podle mě děje poměrně dost chyb, protože lidé mají pocit, že můžou hodit do AI i velmi komplexní tabulky s tisícovkami řádků, ale je potřeba opravdu vědět, kdy to můžu hodit do chatbota, nebo spíš si nechat napsat nějaký skript, kdy musím použít specializovaný nástroj. Takže pro tu jednorázovou práci je dobré znát možnosti a vědět, kdy jaký nástroj použít. Pro pravidelnou práci s daty tak už je dobré napsat si nějakou automatizaci nebo použít specializované nástroje, abychom ta data měli na jednom místě, měli od nich přehled a měli i o těch datech z nich bavit. Takže pro tu první kategorii tam si vystačíte úplně s těmi nástroji, které už používáte. Pro tu druhou mám pro vás několik doporučení, na jaké nové nástroje se můžete podívat. Jak můžeme s daty pracovat? Jednak někam ukládáme, potom je zpracováváme a nakonec využíváme. A tady ty tři scénáře, ukládání, zpracování a využití, vám už určitě něco připomínají. A to téma druhého mozku. I tam jsme se bavili o ukládání informací, o zpracování pro lidi nebo pro jehaj a o jejich praktické využití. No a data vlastně nejsou nic jiného než informace. Akorát v tomto případě se spíš budeme bavit o tom, jak pracovat s těmi daty, které jsou třeba v tabulkách a nebo jak pracovat s daty, kterých je hodně. Jinak je to téma, který je hodně spojený s tím druhým mozgem. Já jsem tady v té oblasti docela dost pokročil, takže vám ukážu zase nějaké velmi zajímavé scénáře a poznatky, jak ta data do druhého mozgu dostat. Nicméně je tady i pár scénářů, které jsme do dnešního dne neřešili, takže si je pojďme projet a já vám ukážu, jaké možnosti nám dnes digitální nástroje pro práci s daty nabízejí. Začal bych s získáváním dat. Tady na obrázku v prezentaci vidíte grafiku, kterou měli na webu přátelé ze startupu Kebula, o kterém ještě dneska budu mluvit. Je to takový mix aplikací, které někdy potřebují sledovat marketéři k tomu, aby dokázali vyhodnocovat marketingové obchodní kampaně. Ale tak to častokrát vypadá jakákoliv firma, protože máme data v různých systémech a buď ty systémy dokážou spolumluvit, to znamená, že je tam nějaké napojení, nebo se vytváří nějaké mezivrství. Případně to, co dneska můžeme asi všichni udělat skoro v jakémkoliv systému, pokud chci pracovat s daty jednorázově, je nechat si data vyexportovat. Tady mám zrovna ukázku exportu jednak z Notionu. To znamená, že pokud třeba byste chtěli analyzovat kompletně vaše poznámky v Notionu, tak buď můžete napojit AI na Notion a nebo si vyexportujete ty záznamy a pak je analyzujete v dalším nástroji. Druhý obrázek, který vidíte, je export s e-mailu pro Google aplikace. Říká se tomu Google Takeout. Microsoft má určitě takovou možnost a dá se to třeba využít, když chcete analyzovat příchozí konverzace od zákazníků nebo chcete třeba vytáhnout od obchodníků veškeré e-maily, se kterými komunikovali a o čem chcete aktualizovat, obohatit CRM-ko, tak už dneska můžete poměrně jednoduše vyexportovat data a potom napsat nějaký skrypt, který je pomáhá analyzovat. Takže export dat z vašich stávajících systémů. Ta třetí možnost je takzvaný web scraping, který jsme si už taky ukazovali. To znamená, že mám nástroje, které mi dokážou stahovat data z internetu, s tím, že to můžou dělat buď jednorázově nebo pravidelně a některá už data dělají takzvaně LLM ready. To znamená, jsou přizpůsobená AI aplikacím a myslím si, že celkově tady těch nástrojů, které umožňují, abych si vzal data z třetích stran ať už web scrapingem nebo napojím přes takzvané API, tak bude čím dál víc. Takže získávání dat je taková první kategorie. Ta druhá kategorie je ukládání dat, kde máme pořád ještě staré dobré tabulky, které čím dál více integrují umělou inteligenci. Pořád je to pro řadu lidí ta nejjednodušší forma, jak si můžu dát data do přehledné database. To není ani databáza, ale zkrátka tabulky a následně s nimi pracovat. Proč je to důležité? Je to zásadní nejen pro přehlednou práci a o tom, že nástroje jako Excel nebo Google Sheets jsou hodně flexibilní, proto jsou tak oblíbené, ale tak je to základ toho, abychom třeba dokázali nad těmi daty stavit automatizace nebo používat AI. Takže základem jsou tabulky a potom takovou další kategorii jsou už databáze a teď nemyslím úplně ajťácké, ale spíš takové napomezí. Když se podíváte na nástroje jako je Microsoft Lists nebo Airtable, který já osobně na který nedám dopustit a nebo používáte Notion a tam si vlastně otevřete databases, tak ty už umí daleko víc než ty tabulky. To znamená, můžete si tam ukládat nejenom textové parametry, ale třeba soubory, různé jiné formáty, můžete ty tabulky spoluprovazovat. To znamená, je to taková další generace práce s daty a pokud chcete automatizovat trochu více, tak se bez tady těch databází už neobejdete. Ale i oni už začínají v sobě mít ty AI funkcionality. Takže to, co doporučuji, podívat se třeba na Microsoft Lists nebo na Airtable, ukázat si šablony, které tam jsou a popřemýšlet, jestli byste třeba podobně nemohli mít zorganizované i nějaké vaše projekty. Co se týče zpracování dat, já se k tomu ještě dostanu trochu víc do detailů, ale tady máme nově poměrně velké možnosti s nástrojích, které máme všichni k dispozici. Ještě před pár měsíci s tabulkami a trošku pokročilějšími daty umělo pracovat jen pár nejpokročilějších modelů. Dnes mohu nahrát tabulku víceméně do jakéhokoliv nástroje, případně už tam nahrát i nějaké věci, které jsou v nástroji. Takže ještě před pár měsíců...

Nějaká komplexnější data, nejpokročilější je v tom zatím CLAUDE, který má ten model i hodně přizpůsobený na finance, analýzu, tabulek různých. Ale dá se říct, že jakýkoliv dobrý placenej model, už vám dneska dokáže poměrně dobře data zpracovávat. A to nejenom tak, že se na ně podívá a uplatní na ně ten velký jazykový model, ale že si přímo v něm napíše, naprogramuje nějaký skript a ten potom ta data zpracovává. Takže to je pro řadu lidí dneska nejjednodušší cesta, Copilot, Analyst, CLAUDE či GPT, ale musíme vědět, kdy je dobré to použít a kdy naopak potřebujeme použít už trošku pokročilější nástroje. Ta druhá možnost je pro ty z vás, kteří už vyzkoušeli napojit třeba CLAUDE anebo GPT přes tzv. MCP server, protože čím dál více specializovaných datových nástrojů má to napojení přes tento MCP server. To znamená, že vy potom, když máte ta data na jednou místě, tak vy se můžete na ně napojit přes váš CLAUDE, kurzor nebo teďka nově relativně i chat GPT a můžete se s těmi daty bavit. Zatím jsme relativně na začátku, ale já osobně si myslím, že toto je budoucnost práce s daty. Že nebudete naklikávat různé reporty, dashboardy, ale vlastně zeptáte se na to, co vás zajímá a nebo se necháte překvapit a AI vám řekne, co v těch datech je zajímavého. Takže i proto je tady ta oblast fascinující a mě osobně velmi baví dívat se, kam se to posouvá, protože já úplně nejsem na nějaké sledování čísel a tabulek, ale v momentě, kdy toto mám napojené na AI, tak s tím pracuji desetkrát víc, než kdybych měl pracovat jenom s Excelem. Čtvrtá kategorie je reporting. Tady nebudu zabíhat do detailu, protože je to už opravdu... Myslím si, že není to nic nového. Nástroje jako je Microsoft Power BI nebo Tableau, Looker Studio, kde si můžete zobrazit vaše data. Objevují se i další, která v sobě začínají zahrnovat AI funkce, ať už takové, že se právě můžete na ta data ptát, a nebo že vám postaví ty dashboardy tím, že si napíšete, co vás zajímá. Nicméně pro některé typy aktivit je pořád nejvhodnější používat ty tradiční datově analitické a business intelligence nástroje. Nicméně pro ty z vás, kteří vyzkoušeli wipecoding, zkusili si třeba postavit nějakou jednoduchou aplikaci a chtěli by proskoumat i možnosti analýzy dat, tak samozřejmě wipecoding i tady nabízí možnost automatizovat si tu datovou analitiku minimálně na těch, řekl bych, jednodušších a středně komplexních projektech. Typicky to vypadá tak, že pokud třeba chcete analyzovat nějaká data, můžete vyzkoušet nástroj Collab od Google, nebo nějaké takové datové notebooky, které máte i v nástroji z minulého modulu. AI vám napíše buď celý skript, to znamená, nahrájete tam data a AI vám je analyzuje, vytvoří vám dashboardy, podobně jak je viděte i na tom videu, nebo si ten skript budete nechat stavit někde vedle v chat GPT a CLAUDE a budete si ho spouštět v Excelu, i tam budou spouštět skrypty. AI vytváří skrypty a ty pak můžete spouštět na vašich datech a bude analyzovat nebo si z toho třeba vytáhávat nějaké zajímavé insighty. Opět i tady v té kategorii se to přibližuje lidem, přibližuje se to biznisu, což je výhoda a je dobré vidět nejenom tady ty možnosti, ale vyzkoušet si to ideálně na nějakém vlastním příkladu. No a teď jaké máme možnosti na to všechno využívat AI, kromě tady toho, co jsme si ukazovali. Já jsem už když přišel model O1 chat GPT, který uměl poprvé trošku více analyzovat data, tak jsem tím byl úplně fascinovaný a hned jsem to vyzkoušel. Chtěl jsem si analyzovat diskuse s chat GPT, to ještě jsem neměl kurzor ani jsem vůbec nevěděl, jak mám spustit někde nějaký skrypt. A takže jsem se s chat GPT bavil o tom, co chci dosáhnout. To znamená, tady jsem chtěl udělat jednak souhrn a hlavně jsem chtěl zařadit všechny ty konverzace, chtěl jsem zjistit, co řeším nejčastěji, jak pracuji, jak přemýšlím a potom udělat nějaký systém, kdybych se mohl zpětně do minulosti dostat tomu, co jsem zrovna řešil. Bylo strašně zajímavé, že AI mě provádělo celým tím systémem, někdy mi tam doporučoval i takové, řekl bych, opravdu datařské aplikace. No a na konci tady té diskuze, když jsem ten základ udělal, tak mi řekl jednu takovou zajímavou věc. On přímo nakonec mi říkal, krok za krokem uvidí, že s Pythonem, se skryptem a s AI odmakáš 90% datové analitiky bez toho, aby jsi byl data scientist, datový vědec. No a já jsem si uvědomil, že i tady platí, kde jinde, že i AI nám dává dovednosti, které nemáme, takže najednou opravdu můžeme analyzovat poměrně rozsáhlá data, aniž bychom opravdu těmi datovými věci byli. A toto mimochodem je velmi, velmi starý příklad, myslím si, že ze začátku roku 2024. Takže dneska těch možností máme opravdu hodně. Jak jsem říkal, práce s daty se hodně posouvá od tabulek a manuální práce přes reporty, dashboardy v takových těch nástrojích, jako je Power BI, které vizualizovaly data, dávaly nám přehledy, mohli jste si to proklikávat. Až po tu novou generaci, která používá přirozený jazyk, dostanete okamžitě insights a talk to data. To znamená, bavíte se ve spolupráci s AI s vašimi daty. Mimochodem tu infografiku mi dělal nový model Gemini Pro od Google, který umí teda už jako velmi hezky generovat i texty a infografiky. Takže jenom takový tip bokem. Nicméně, toto je podle mě taková evoluce. Naši přátelé v Kebule vyvíjejí ten systém na talk to data a Petr Šimeček, jeden ze spolu zakladatelů, publikuje pravidelně na LinkedInu i na svém blogu velmi zajímavé články o tom, jak se bude datová analitika vyvíjet. Řada lidí potvrzuje, že klasická business inteligence, jak jí známe, tak nebude existovat, protože nebude dávat smysl, aby existovalo ve firmě stovky reportů na různé aktivity, ale opravdu je budete dostávat buď v reálném čase nebo jak si řeknete. I proto někdy říkají, že business intelligence je vlastně mrtvá a je pravda, že do budoucna ta data budou opět sloužit spíš vašim agentům nebo asistentům, protože ti budou chtít vlastně dělat třeba rozhodnutí na datách nebo vy se budete ptát svého agenta na nějaké informace, on pak se dostane k nějakým datům, analizuje je a řekne vám je zpátky. No a toto je vlastně takový nový koncept, kdy budeme muset do budoucna přizpůsobovat ty naše systémy právě tomu, aby jim data, respektive agenti, rozuměli. Když mluvím o kebule, tak toto je za mě jeden z nejlepších příkladů a ukázka toho, jak vypadá práce s daty, pokud máte data dobře poskládaná, máte chytrný přístup a používáte ty nejmodernější nástroje. Jak vidíte, někdo se zeptal na analýzu trafiku, to znamená webového marketingu. Uživatel řekl v Claudovi, který byl napojený na kebulu, že chce analyzovat zdroje návštěvnosti na internetových stránek. AI to zanalyzovala, v momentě, kdy něco nefungovalo, tak se omluvila a začala pracovat na trošku jiné metodě. Na konci uživatel řekl, udělej mi dashboard. Vidíte, že AI programuje dashboard a na konci z toho vyhodí. Tak to vypadá a bude vypadat práce s daty a my se budeme bavit o tom, jak se k tomu dokážeme dostat. Jenom prosím, dejme si pozor na jednu věc. Jak jsem už zmiňoval, firmy často slibují, že práce s daty bude daleko jednodušší, než se zdá. Tady ta demo Microsoft Copilotu už se objevila před rokem, nebo možná rokem a půl v roce 2024 a pořád ta spolehlivost Copilota v Excelu není stoprocentní. Takže musíme si na to dát pozor a musíme se opravdu pečlivě zkoušet a zjišťovat, kdy já to AI můžu v datech použít a kdy já naopak bych měl použít nějaké další metody. To, co dneska funguje, to, co já dneska doporučuji, je jednak pro jednoduché analýzy stačí nahrát soubor buď tabulku v Excelu, nebo si to vyexportovat jako CSV do chatbotů, mít tam zapnutý ten reasoning mode, to znamená thinking a zkrátka zeptat se. Buď se ptát velmi konkrétně, nebo naopak obecně říct, řekni mi, co je v těchto datech a funguje to už velmi dobře. Objevují se i AI funkce v tabulkách přímo, to znamená, ať už Gemini nebo Copilot už dneska mají funkce, kde otevřete tabulku a můžete se bavit. Někdy to funguje, někdy ne. Nicméně kromě toho se ještě objevují funkce, kde vy do buňky zadáte podobně, jako zadáváte vzorce, tak zadáte funkci buď AI nebo Gemini nebo Copilot a pak tam napíšete prompt a můžete třeba odkázat na další buňky. A to, co se dá krásně využít, když třeba máte 150 e-mailů od zákazníků nebo nějakých zpětných vazeb v tabulce a chcete, aby vám AI u každé udělali překlad do anglištiny a třeba zhrnutí do jedné věty. A vy napíšete prompt, udělej mi zhrnutí tohoto textu a udělej to v anglištině, odkážete na tu buňku no a potom dáte jen copy-paste a můžete to klidně udělat pro 150 buňek podobně jako jakékoliv jiné vzorce v tabulce. Takže najednou toto otevírá spoustu možností, jak s těmi tabulkami pracovat a objevují se i velmi zajímavé specializované aplikace, jako je GPT4Sheets nebo Ramp, kde opravdu už jako v těch tabulkách umí ti agenti dělat velmi pokročilé věci. Třetí možnost AI, která generuje skripty, to znamená složitější tabulky. Mám pět tabulek, chci analizovat, navrhnout sloučení. Toto už nezvládnou běžné chatboty, už potřebujete napsat skript, ale naštěstí máme tady wipecoding, takže můžete poprosit CLAUDE Code nebo Cursor nebo jakoukoliv další technologii, aby vám je napsala. Buď si to hodíte do některých specializovaných nástrojů, nebo to uděláte přímo v Cursoru, CLAUDE Code nebo Google Apps Script, který ta data zvládne velmi dobře. No a jak jsem říkal, specializované nástroje, Pseda, Pandas, Kebula, dokážou dneska napojit databáze a bavit se přirozeným jazykem. Já osobně, všem, kteří to myslí s daty a s analýzou a s reportingem vážně, tak by se měli potkat s Kebulou, je to český startup, investoval do něj mimo jiné Tomáš Čupr z Rohlíku, celý Rohlík na tom jede, česká spořitelna na tom jede a je to podle mě opravdu nejen budoucnost datové analytiky, ale hlavně agentů, protože jakmile máte vrstvu, kde máte přístup ke všem datům, které ve firmě máte, tak nejenom, že si z toho vlastně děláte různé reporty a dashboardy, ale hlavně můžete právě do toho pustit agenty na automatizaci. Já pak tady pro vás mám ještě další typy na nástroje, ať už na různé asistenty, frameworky na wipecoding, různé platformy na sběr obohacování dat, takže doporučuji, pokud vás to téma zajímá, tak si to projet, proklikat, podívat se na ty možnosti a ideálně vlastně neřešit, že třeba zatím ještě AI nepoužíváte na práci s daty. Já si myslím, že všichni jsou na začátku, je dobré podívat se na možnosti, když se podíváte na ty apky, tak prostě vidíte ty scénáře a začne se vám to spojovat s tím, co byste mohli potřebovat. Nevždy je potřeba řešit nějaký komplikovaný nástroj, někdy je opravdu nejjednoduší vyexportovat si to do CSVčka nebo si hodit texty do notebooku LM a máte to hotovo, ale přemýšlet už na tím, co byste potřebali mít dlouhodobě v tom druhém mozku a asi začít se připravovat na tu dobu AI agentů, aby měli ta správná data. Poslední bod, akce přináší informace, takže můžeme dlouho přemýšlet, jak druhý mozek postavit, jak analyzovat to, či ono. Nejlepší je začít co nejdřív, začít si s datama hrát, zkoušet a během toho zjišťovat. Pokud děláte ve větší firmě nebo máte nějaký větší projekt, tak udělat nějaké průzkum v konci, pilotní projekt a to vám pak ukáže cestu, kudy se vydat. Mimochodem, já už jsem zmiňoval, pokud si myslíte, že nemáte kvalitní data, kontakty na zákazníky, cokoliv, je nejlepší příležitost udělat si třeba v lednu, kdy je trošku voněj čištění, sprint, hekaton, když zkusíte ta data za pomocí AI vyčistit, připravit. Tady to není úplně populární práce, většinou to nepatří mezi oblíbené aktivití lidí ve firmách, ale když se to udělá pořádně, použije se k tomu AI, tak se to dá zvládnout za velmi krátký čas, nebo minimálně kratší než obvykle. No a mám pro vás takový menší úkol, pokud chcete být The Future AI Leader, je dobré udělat si nějakou analýzu strukturovaných nebo nestrukturovaných dat, takže mám tady takové tři typy na úkoly. Pokud nevíte, co nemáte data, můžete si zkusit vyexportovat třeba LinkedIn posty nebo nějaké vaše e-maily a najít v tom vzorce, případně si jenom skopírujte posledních 20 e-mailů, nechte si to zanalizovat a na základě toho si třeba udějte návrh, promptu na asistenta. Pokud máte data, zkuste je hodit do AI, zkuste k tomu využít různé, samozřejmě s ohledem na bezpečnost a buď se zeptejte, co zajímavého vidíš v těchto datech, nebo zkuste si klást konkrétní otázky. A pokud máte trošku větší ambice, zkuste wipecoding, to znamená zkuste si vzít nějaké větší třeba datasety, tabulky, větší dokumenty a pobavte se s AI, jak vám může napsat skript pro automatickou analýzu nebo analýzu opravdu většího množství informací, na které třeba AI nestačí. Takže toto je práce s daty. V pátek se podíváme na velmi konkrétní příklady toho, jak já třeba analyzuji data, pracuju s nimi a budeme si ukazovat, jak se dá pracovat s těmi nástroji, jako je třeba AI v tabulkách, personalizace, větší množství kontaktů a opravdu něco, co si myslím, že vám ukáže všech ty možnosti a bude vás to inspirovat i pro vlastní práci. Takže ještě jednou díky moc a v pátek naviděnou. Mějte se.



